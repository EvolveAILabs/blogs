{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0755664-d1aa-408d-86ca-dd22066b06c2",
   "metadata": {},
   "source": [
    "# CI/CD in Generative AI Development Series\n",
    "## LLM Stability Testing with Python Unittests \n",
    "\n",
    "This notebook showcases how to run Stability Tests using unit testing frameworks available in python. Stability testing is important in Generative AI applications especially the ones using LLM APIs like OpenAI, Anthropic, Gemini, BedRock etc.,. The LLMs behind the APIs are frequently updated and hot swapped leading to change in performance of the LLM. Consumers of these APIs should have automated stability testing frameworks to intercept any drift in LLM output. Stability testing for Generative AI acts as the canary in the coal mine for these consumer applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd94e1d-1abd-4da9-a499-152eb2ce4fe6",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "\n",
    "An process automation application that uses an LLM to ingest bills from users and scrapes data from them to upload to a backend office application. The stability of the GPT-4o endpoint in this scenario is its ability to extract correct information from the bills over time. To make sure that the performance of the LLM is not falling, developers will need to write tests that measure the performance of the LLM over an evaluation dataset and provide alerts if the performance tests fail. This is quite similar to unit testing in CI/CD for software development. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5809c58-21f8-4141-bc0c-ac57b8bc0da2",
   "metadata": {},
   "source": [
    "### Input Data\n",
    "\n",
    "For this showcase, we are using the kaggle dataset provided by the use ------. To test the stability of our LLM we have created an evaluation dataset that has a input and expected output and can be used to test if the LLM is stable or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "15e9ad5c-1bbf-4399-9d8a-d210851ce4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7 entries, 0 to 6\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   filename  7 non-null      object\n",
      " 1   output    7 non-null      object\n",
      "dtypes: object(2)\n",
      "memory usage: 244.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "llm_test_data = pd.DataFrame({'filename':['chowderhut_20231005_011.pdf','shell_20231005_003.pdf',\n",
    "                                          'beerhouse_20231209_005.pdf','dennys_20231209_004.pdf',\n",
    "                                          'cafemason_20231005_009.pdf','topgolf_20231209_011.pdf',\n",
    "                                          'yellow_20231209_008.pdf'],\n",
    "                              'output':[{'category': 'food', 'total_bill_amount': 21.15},\n",
    "                                        {'category': 'transport', 'total_bill_amount': 28.32},\n",
    "                                        {'category': 'food', 'total_bill_amount': 41.72},\n",
    "                                        {'category': 'food', 'total_bill_amount': 58.44},\n",
    "                                        {'category': 'food', 'total_bill_amount': 32.59},\n",
    "                                        {'category': 'other', 'total_bill_amount': 155.68},\n",
    "                                        {'category': 'transport', 'total_bill_amount': 43.02},\n",
    "                                       ]\n",
    "                             })\n",
    "llm_test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "53ca3ecf-75ae-4e4e-a3a1-a27d3d3820fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chowderhut_20231005_011.pdf</td>\n",
       "      <td>{'category': 'food', 'total_bill_amount': 21.15}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shell_20231005_003.pdf</td>\n",
       "      <td>{'category': 'transport', 'total_bill_amount':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>beerhouse_20231209_005.pdf</td>\n",
       "      <td>{'category': 'food', 'total_bill_amount': 41.72}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dennys_20231209_004.pdf</td>\n",
       "      <td>{'category': 'food', 'total_bill_amount': 58.44}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cafemason_20231005_009.pdf</td>\n",
       "      <td>{'category': 'food', 'total_bill_amount': 32.59}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      filename  \\\n",
       "0  chowderhut_20231005_011.pdf   \n",
       "1       shell_20231005_003.pdf   \n",
       "2   beerhouse_20231209_005.pdf   \n",
       "3      dennys_20231209_004.pdf   \n",
       "4   cafemason_20231005_009.pdf   \n",
       "\n",
       "                                              output  \n",
       "0   {'category': 'food', 'total_bill_amount': 21.15}  \n",
       "1  {'category': 'transport', 'total_bill_amount':...  \n",
       "2   {'category': 'food', 'total_bill_amount': 41.72}  \n",
       "3   {'category': 'food', 'total_bill_amount': 58.44}  \n",
       "4   {'category': 'food', 'total_bill_amount': 32.59}  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a1dce1-250f-4f1f-a2bb-44cc28e74c35",
   "metadata": {},
   "source": [
    "### LLM Function\n",
    "The LLM completion function to be tested. This function simulates the endpoint which will ingest text from a bill to be reimbursed and return the bill amount and the category of the bill. The function uses GPT-4o model as the LLM but this notebook can be used with any LLM API or self hosted LLM as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd1d09db-970a-495c-b7ed-759a28d20e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pymupdf4llm\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "key = json.load(open('./openai_key.json'))['key']\n",
    "\n",
    "def prompt_llm(bill_text, key):\n",
    "    client = OpenAI(api_key=key)\n",
    "    completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assistant, skilled in helping \\\n",
    "         extract billing information from invoices and bills to help in reimbursement processes.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Using the following extracted information from a bill in triple quotes, generate \\\n",
    "          total_bill_amount, category which is one of food, transport, other as output. \\\n",
    "         Output only in json format as {\\\"total_bill_amount\\\":, \\\"category\\\":}. \\\n",
    "         \\n Bill Extract: \\n \\n \\n '''\" + bill_text + \"'''\"}\n",
    "      ],\n",
    "      seed = 42\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffdec0a-f465-4ce6-b286-fee42ced7d3e",
   "metadata": {},
   "source": [
    "### Stability Tests using Python Unittest\n",
    "\n",
    "Python's [unittest library](https://docs.python.org/3/library/unittest.html) is the defacto unit testing library that is included in the python distribution. Unittest recommends a class based approach to unit testing functions. All the tests to be executed need to be wrapped inside the [TestCase class](https://docs.python.org/3/library/unittest.html#unittest.TestCase) and should start with \"test\" in the name.\n",
    "\n",
    "In our scenario, we will include three test cases with varying levels of stability checks on the LLM API. The tests will be executed in the required pipelines to ensure that the LLM is stable and no interventions are required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0087f031-1a3e-47dc-8aea-624729255b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class LLMStability(unittest.TestCase):\n",
    "        \n",
    "    def test_one_row(self):\n",
    "        # Testing a single row\n",
    "        row = llm_test_data.iloc[0]\n",
    "        md_text = pymupdf4llm.to_markdown(\"./bills/\"+row['filename'])\n",
    "        response = prompt_llm(md_text, key)\n",
    "        response_json = json.loads(response[response.find('{'):response.rfind('}')+1])\n",
    "        self.assertDictEqual(response_json, {'category': 'food', 'total_bill_amount': 21.15})\n",
    "\n",
    "    def test_llm_output_sample(self):\n",
    "        # Testing a sample of rows\n",
    "        for i,row in llm_test_data.head(3).iterrows():\n",
    "            md_text = pymupdf4llm.to_markdown(\"./bills/\"+row['filename'])\n",
    "            response = prompt_llm(md_text, key)\n",
    "            response_json = json.loads(response[response.find('{'):response.rfind('}')+1])\n",
    "            self.assertDictEqual(response_json, row['output'])\n",
    "    \n",
    "    def test_llm_output_full(self):\n",
    "        ROWS_CORRECT = 0\n",
    "        EXPECTED_MATCH_RATE = 0.5\n",
    "        for i,row in llm_test_data.iterrows():\n",
    "            # Testing entire evaluation dataset and checking threshold of at least 50% match rate\n",
    "            md_text = pymupdf4llm.to_markdown(\"./bills/\"+row['filename'])\n",
    "            response = prompt_llm(md_text, key)\n",
    "            response_json = json.loads(response[response.find('{'):response.rfind('}')+1])\n",
    "            if(response_json == row['output']):\n",
    "                ROWS_CORRECT=ROWS_CORRECT+1\n",
    "        self.assertGreaterEqual(ROWS_CORRECT/llm_test_data.shape[0],EXPECTED_MATCH_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "75c10353-a8b4-4afe-9509-986e5f66361f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 10.343s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "result = unittest.main(argv=[''], verbosity=1, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "882e4eb1-1848-4348-9bcc-9b8a3fee590e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results \n",
      "------------------------------\n",
      "Total Tests Executed: 3\n",
      "Tests Failed: 0\n",
      "Test Execution Run Success: True\n"
     ]
    }
   ],
   "source": [
    "print('Test Results \\n'+''.join(['-' for i in range(30)])+'\\nTotal Tests Executed: {}\\nTests Failed: {}\\nTest Execution Run Success: {}'.format(\n",
    "    result.result.testsRun,\n",
    "    len(result.result.failures),\n",
    "    result.result.wasSuccessful()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7257b175-4570-4f62-84b0-4db5361799c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5080dd5c-c972-48c5-83e3-dabedcb99616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86d8e203-95f1-4014-8953-491a31a03c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_my_stuff():\n",
    "    assert 42 == 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b380f95a-5de5-4d9e-bd74-b4a9df543f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33m                                                                                         [100%]\u001b[0m\n",
      "\u001b[33m========================================= warnings summary =========================================\u001b[0m\n",
      "../../../../../../opt/anaconda3/envs/devenv311/lib/python3.11/site-packages/_pytest/config/__init__.py:1277\n",
      "  /opt/anaconda3/envs/devenv311/lib/python3.11/site-packages/_pytest/config/__init__.py:1277: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: anyio\n",
      "    self._mark_plugins_for_rewrite(hook)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m\u001b[32m4 passed\u001b[0m, \u001b[33m\u001b[1m1 warning\u001b[0m\u001b[33m in 13.11s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = ipytest.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2cbeac8b-1c35-4f3d-a7e0-cb16185072e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.TESTS_FAILED.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "79a73c57-2c73-45e4-b721-30cc23033061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.NO_TESTS_COLLECTED.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0a3a12a-4870-4484-b039-d2e1eef5a8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.OK.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0d55b6-520e-4cbf-9ae3-c3868880ca4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
