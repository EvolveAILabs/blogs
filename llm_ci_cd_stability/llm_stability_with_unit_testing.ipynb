{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0755664-d1aa-408d-86ca-dd22066b06c2",
   "metadata": {},
   "source": [
    "# CI/CD in Generative AI Development Series\n",
    "## LLM Stability Testing with Python unit testing frameworks \n",
    "\n",
    "This notebook showcases how to run Stability Tests using unit testing frameworks available in python. Stability testing is important in Generative AI applications especially the ones using LLM APIs like OpenAI, Anthropic, Gemini, BedRock etc.,. The LLMs behind the APIs are frequently updated and hot swapped leading to change in performance of the LLMs which are deployed in production applications of many organizations. Consumers of these APIs should have automated stability testing frameworks to intercept any drift in LLM behaviour/output. Stability testing for Generative AI acts as the canary in the coal mine for these critical applications consumed by organizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd94e1d-1abd-4da9-a499-152eb2ce4fe6",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "\n",
    "An process automation application that uses an LLM to ingest bills from users and scrapes data from them to upload to a backend office application. The stability of the GPT-4o endpoint in this scenario is its ability to extract correct information from the bills over time. To make sure that the performance of the LLM is not falling, developers will need to write tests that measure the performance of the LLM over an evaluation dataset and provide alerts if the performance tests fail. This is quite similar to unit testing in CI/CD for software development. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5809c58-21f8-4141-bc0c-ac57b8bc0da2",
   "metadata": {},
   "source": [
    "### Input Data\n",
    "\n",
    "For this showcase, we are using the kaggle dataset provided by the use ------. To test the stability of our LLM we have created an evaluation dataset that has a input and expected output and can be used to test if the LLM is stable or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8453fb8e-2e44-43d0-bb88-a55e4278de69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pymupdf4llm\n",
    "import pandas as pd\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15e9ad5c-1bbf-4399-9d8a-d210851ce4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7 entries, 0 to 6\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   filename  7 non-null      object\n",
      " 1   output    7 non-null      object\n",
      "dtypes: object(2)\n",
      "memory usage: 244.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "llm_test_data = pd.DataFrame({'filename':['chowderhut_20231005_011.pdf','shell_20231005_003.pdf',\n",
    "                                          'beerhouse_20231209_005.pdf','dennys_20231209_004.pdf',\n",
    "                                          'cafemason_20231005_009.pdf','topgolf_20231209_011.pdf',\n",
    "                                          'yellow_20231209_008.pdf'],\n",
    "                              'output':[{'category': 'food', 'total_bill_amount': 21.15},\n",
    "                                        {'category': 'transport', 'total_bill_amount': 28.32},\n",
    "                                        {'category': 'food', 'total_bill_amount': 41.72},\n",
    "                                        {'category': 'food', 'total_bill_amount': 58.44},\n",
    "                                        {'category': 'food', 'total_bill_amount': 32.59},\n",
    "                                        {'category': 'other', 'total_bill_amount': 155.68},\n",
    "                                        {'category': 'transport', 'total_bill_amount': 43.02},\n",
    "                                       ]\n",
    "                             })\n",
    "llm_test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53ca3ecf-75ae-4e4e-a3a1-a27d3d3820fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chowderhut_20231005_011.pdf</td>\n",
       "      <td>{'category': 'food', 'total_bill_amount': 21.15}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shell_20231005_003.pdf</td>\n",
       "      <td>{'category': 'transport', 'total_bill_amount':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>beerhouse_20231209_005.pdf</td>\n",
       "      <td>{'category': 'food', 'total_bill_amount': 41.72}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dennys_20231209_004.pdf</td>\n",
       "      <td>{'category': 'food', 'total_bill_amount': 58.44}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cafemason_20231005_009.pdf</td>\n",
       "      <td>{'category': 'food', 'total_bill_amount': 32.59}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      filename  \\\n",
       "0  chowderhut_20231005_011.pdf   \n",
       "1       shell_20231005_003.pdf   \n",
       "2   beerhouse_20231209_005.pdf   \n",
       "3      dennys_20231209_004.pdf   \n",
       "4   cafemason_20231005_009.pdf   \n",
       "\n",
       "                                              output  \n",
       "0   {'category': 'food', 'total_bill_amount': 21.15}  \n",
       "1  {'category': 'transport', 'total_bill_amount':...  \n",
       "2   {'category': 'food', 'total_bill_amount': 41.72}  \n",
       "3   {'category': 'food', 'total_bill_amount': 58.44}  \n",
       "4   {'category': 'food', 'total_bill_amount': 32.59}  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a1dce1-250f-4f1f-a2bb-44cc28e74c35",
   "metadata": {},
   "source": [
    "### LLM Function\n",
    "The LLM completion function to be tested. This function simulates the endpoint which will ingest text from a bill to be reimbursed and return the bill amount and the category of the bill. The function uses GPT-4o model as the LLM but this notebook can be used with any LLM API or self hosted LLM as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd1d09db-970a-495c-b7ed-759a28d20e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = json.load(open('./openai_key.json'))['key']\n",
    "\n",
    "def prompt_llm(bill_text, key):\n",
    "    client = OpenAI(api_key=key)\n",
    "    completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assistant, skilled in helping \\\n",
    "         extract billing information from invoices and bills to help in reimbursement processes.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Using the following extracted information from a bill in triple quotes, generate \\\n",
    "          total_bill_amount, category which is one of food, transport, other as output. \\\n",
    "         Output only in json format as {\\\"total_bill_amount\\\":, \\\"category\\\":}. \\\n",
    "         \\n Bill Extract: \\n \\n \\n '''\" + bill_text + \"'''\"}\n",
    "      ],\n",
    "      seed = 42\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffdec0a-f465-4ce6-b286-fee42ced7d3e",
   "metadata": {},
   "source": [
    "### Stability Tests using Python Unittest\n",
    "\n",
    "Python's [unittest library](https://docs.python.org/3/library/unittest.html) is the defacto unit testing library that is included in the python distribution. Unittest recommends a class based approach to unit testing functions. All the tests to be executed need to be wrapped inside the [TestCase class](https://docs.python.org/3/library/unittest.html#unittest.TestCase) and should start with \"test\" in the name.\n",
    "\n",
    "In our scenario, we will include three test cases with varying levels of stability checks on the LLM API. The tests will be executed in the required pipelines to ensure that the LLM is stable and no interventions are required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0087f031-1a3e-47dc-8aea-624729255b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class LLMStability(unittest.TestCase):\n",
    "        \n",
    "    def test_one_row(self):\n",
    "        # Testing a single row\n",
    "        row = llm_test_data.iloc[0]\n",
    "        md_text = pymupdf4llm.to_markdown(\"./bills/\"+row['filename'])\n",
    "        response = prompt_llm(md_text, key)\n",
    "        response_json = json.loads(response[response.find('{'):response.rfind('}')+1])\n",
    "        self.assertDictEqual(response_json, {'category': 'food', 'total_bill_amount': 21.15})\n",
    "\n",
    "    def test_llm_output_sample(self):\n",
    "        # Testing a sample of rows\n",
    "        for i,row in llm_test_data.head(3).iterrows():\n",
    "            md_text = pymupdf4llm.to_markdown(\"./bills/\"+row['filename'])\n",
    "            response = prompt_llm(md_text, key)\n",
    "            response_json = json.loads(response[response.find('{'):response.rfind('}')+1])\n",
    "            self.assertDictEqual(response_json, row['output'])\n",
    "    \n",
    "    def test_llm_output_full(self):\n",
    "        # Testing entire evaluation dataset and checking threshold of at least 50% match rate\n",
    "        ROWS_CORRECT = 0\n",
    "        EXPECTED_MATCH_RATE = 0.5\n",
    "        for i,row in llm_test_data.iterrows():\n",
    "            md_text = pymupdf4llm.to_markdown(\"./bills/\"+row['filename'])\n",
    "            response = prompt_llm(md_text, key)\n",
    "            response_json = json.loads(response[response.find('{'):response.rfind('}')+1])\n",
    "            if(response_json == row['output']):\n",
    "                ROWS_CORRECT=ROWS_CORRECT+1\n",
    "        self.assertGreaterEqual(ROWS_CORRECT/llm_test_data.shape[0],EXPECTED_MATCH_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95531b3f-37dd-42ac-9f9b-0a59d154c6ea",
   "metadata": {},
   "source": [
    "#### Running the Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75c10353-a8b4-4afe-9509-986e5f66361f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_llm_output_full (__main__.LLMStability.test_llm_output_full) ... ok\n",
      "test_llm_output_sample (__main__.LLMStability.test_llm_output_sample) ... ok\n",
      "test_one_row (__main__.LLMStability.test_one_row) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 10.813s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "result = unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aaa15c-6df3-4fe7-9a19-7863ad44c859",
   "metadata": {},
   "source": [
    "#### Test Results\n",
    "All the stability tests are fine and the LLM is stable and no intervention is required. To see what the output looks when a test fails, please update `EXPECTED_MATCH_RATE` to 0.99. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "882e4eb1-1848-4348-9bcc-9b8a3fee590e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Stability Test Results using UNITTESTS Library \n",
      "------------------------------\n",
      "Total Tests Executed: 3\n",
      "Tests Failed: 0\n",
      "Test Execution Run Success: True\n"
     ]
    }
   ],
   "source": [
    "print('LLM Stability Test Results using UNITTESTS Library \\n'+''.join(['-' for i in range(30)])+'\\nTotal Tests Executed: {}\\nTests Failed: {}\\nTest Execution Run Success: {}'.format(\n",
    "    result.result.testsRun,\n",
    "    len(result.result.failures),\n",
    "    result.result.wasSuccessful()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacfa4fd-d554-4ad8-becd-4376dbfcb989",
   "metadata": {},
   "source": [
    "### Stability Tests using Python Pytest\n",
    "\n",
    "[Pytest](https://docs.pytest.org/en/stable/) is a very popular framework amongst its developers because of its versatility of using simple assertions and fixtures. Developers prefer pytest over unittest library because of the influence of Java programming standards in the latter. Both frameworks are robust while being generic and have been used in commercial grade applications of all kinds. The choice comes down to the developer and their preference.\n",
    "\n",
    "We will showcase the same stability tests from the unittest classes using pytest to contrast the difference in the implementation style. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6573977c-6d9d-45ce-9d41-579d1093c5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7257b175-4570-4f62-84b0-4db5361799c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86d8e203-95f1-4014-8953-491a31a03c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_row_pt():\n",
    "    # Testing a single row\n",
    "    row = llm_test_data.iloc[0]\n",
    "    md_text = pymupdf4llm.to_markdown(\"./bills/\"+row['filename'])\n",
    "    response = prompt_llm(md_text, key)\n",
    "    response_json = json.loads(response[response.find('{'):response.rfind('}')+1])\n",
    "    assert response_json == {'category': 'food', 'total_bill_amount': 21.15} \n",
    "\n",
    "def test_llm_output_sample_pt():\n",
    "    # Testing a sample of rows\n",
    "    for i,row in llm_test_data.head(3).iterrows():\n",
    "        md_text = pymupdf4llm.to_markdown(\"./bills/\"+row['filename'])\n",
    "        response = prompt_llm(md_text, key)\n",
    "        response_json = json.loads(response[response.find('{'):response.rfind('}')+1])\n",
    "        assert response_json == row['output']\n",
    "\n",
    "def test_llm_output_full_pt():\n",
    "    # Testing entire evaluation dataset and checking threshold of at least 50% match rate\n",
    "    ROWS_CORRECT = 0\n",
    "    EXPECTED_MATCH_RATE = 0.5\n",
    "    for i,row in llm_test_data.iterrows():\n",
    "        md_text = pymupdf4llm.to_markdown(\"./bills/\"+row['filename'])\n",
    "        response = prompt_llm(md_text, key)\n",
    "        response_json = json.loads(response[response.find('{'):response.rfind('}')+1])\n",
    "        if(response_json == row['output']):\n",
    "            ROWS_CORRECT=ROWS_CORRECT+1\n",
    "    assert ROWS_CORRECT/llm_test_data.shape[0]>EXPECTED_MATCH_RATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894fed55-e080-4eb2-b584-c54a909d12ba",
   "metadata": {},
   "source": [
    "#### Running the Tests\n",
    "\n",
    "PS: You might have noticed that pytest ran 6 tests instead of 3. This is because pytest can recognise tests written in other frameworks and it follows the same naming standards as unittest library. This allows easy migration of unit tests from other frameworks to pytest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b380f95a-5de5-4d9e-bd74-b4a9df543f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
      "platform darwin -- Python 3.11.9, pytest-8.3.3, pluggy-1.5.0 -- /opt/anaconda3/envs/devenv311/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/y2ee201/Documents/evolveailabs/blogs/stability\n",
      "plugins: anyio-4.4.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 6 items\n",
      "\n",
      "t_2cfefb81a85948f0b46123638742dec6.py::LLMStability::test_llm_output_full \u001b[32mPASSED\u001b[0m\u001b[33m             [ 16%]\u001b[0m\n",
      "t_2cfefb81a85948f0b46123638742dec6.py::LLMStability::test_llm_output_sample \u001b[32mPASSED\u001b[0m\u001b[33m           [ 33%]\u001b[0m\n",
      "t_2cfefb81a85948f0b46123638742dec6.py::LLMStability::test_one_row \u001b[32mPASSED\u001b[0m\u001b[33m                     [ 50%]\u001b[0m\n",
      "t_2cfefb81a85948f0b46123638742dec6.py::test_one_row_pt \u001b[32mPASSED\u001b[0m\u001b[33m                                [ 66%]\u001b[0m\n",
      "t_2cfefb81a85948f0b46123638742dec6.py::test_llm_output_sample_pt \u001b[32mPASSED\u001b[0m\u001b[33m                      [ 83%]\u001b[0m\n",
      "t_2cfefb81a85948f0b46123638742dec6.py::test_llm_output_full_pt \u001b[32mPASSED\u001b[0m\u001b[33m                        [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m========================================= warnings summary =========================================\u001b[0m\n",
      "../../../../../../opt/anaconda3/envs/devenv311/lib/python3.11/site-packages/_pytest/config/__init__.py:1277\n",
      "  /opt/anaconda3/envs/devenv311/lib/python3.11/site-packages/_pytest/config/__init__.py:1277: PytestAssertRewriteWarning: Module already imported so cannot be rewritten: anyio\n",
      "    self._mark_plugins_for_rewrite(hook)\n",
      "\n",
      "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
      "\u001b[33m================================== \u001b[32m6 passed\u001b[0m, \u001b[33m\u001b[1m1 warning\u001b[0m\u001b[33m in 19.27s\u001b[0m\u001b[33m ===================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result_pt = ipytest.run('-vv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012dbb38-c971-4065-ae63-935b7eb02cc0",
   "metadata": {},
   "source": [
    "### Integration into Pipelines\n",
    "\n",
    "Both unittests and pytests allow easy integration into pipelines. If you are already having CI/CD pipelines, you can just have python scripts as .py files and they will be automatically picked up by CI/CD tools like Github Actions, Azure DevOPS etc.,."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed473da-1491-4fda-8d90-6e4dcc708615",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Given the rising adoption of LLMs and Generative AI for consumer and commercial use, it is crucial to keep testing the stability of the LLMs at a high frequency to avoid any surprises in Automation Pipelines that support critical operations like payment reconciliations, data processing, customer support etc.,. Leveraging popular unit testing frameworks along with CI/CD pipelines is one of the quickest ways to implement stability testing of LLMs inside your workflows. \n",
    "\n",
    "We at Evolve AI Labs would love to help you on your transformative journey with Generative AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1344c75b-f926-48fa-b1e5-ccb8b8adaefb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
